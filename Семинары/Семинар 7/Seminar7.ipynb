{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Предобработка данных. Категориальные признаки. Работа с текстами.**\n",
    "\n",
    "Содержание семинара опирается на семинары, проводимые на ФКН. Во второй части работы отчасти использован туториал Kaggle по Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первой части семинара будем использовать немного видоизменённый для наших целей датасет https://archive.ics.uci.edu/ml/datasets/AutoUniv.\n",
    "\n",
    "В нём присутствуют целочисленные, вещественнозначные и категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('table.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['class']\n",
    "X = df.drop(['class'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_enc = preprocessing.LabelEncoder()\n",
    "y = label_enc.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преобразование категориальных признаков**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько типов предобработки данных. Она важна для корректной работы различных методов машинного обучения, в особенности, для линейных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в наших данных присутствуют категориальные признаки, то есть текстовые или порядковые признаки - те признаки, к которым нельзя относиться как к числам, то для дальнешей работы с такими данными надо преобразовать категориальные признаки в числа. \n",
    "\n",
    "Нельзя упускать из вида порядковые признаки. Например, если наши данные содержат в качестве столбца индекс местности, то алгоритм будет считать, что индекс 119331 > 119101, что для нас смысла не имеет. Мы хотим, чтобы различные индексы служили индикаторами различных географических областей. Для этого порядковые признаки также надо предобрабатывать и переводить в числовые."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие стоблцы нашей таблицы содержат категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_mask = (X.dtypes == \"object\").values\n",
    "cat_features_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_features_mask[cat_features_mask==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 способ кодирования: счётчики**\n",
    "\n",
    "Мы можем посчитать, сколько раз каждое значение встречалось в таблице и заменить каждый категориальный признак соответствующим счетчиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 способ кодирования: OneHot-кодирование**\n",
    "\n",
    "Пусть некоторый признак принимает значения из множества K. OneHotEncoder вместо одного признака создает K бинарных признаков - по одному на каждое возможное значение исходного признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(sparse=False, drop='first')\n",
    "X_cat = enc.fit_transform(X_copy[X_copy.columns[cat_features_mask]])\n",
    "X_cat = pd.DataFrame(data=X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cat.shape)\n",
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 способ кодирования: хэширование**\n",
    "\n",
    "HashingVectorizer преобразовывает строку в числовой массив заданной длиной с помощью хэш-функции. В этом методе в качестве входных параметров мы задаем желаемое количество новых признаков, а также токенизатор - обработчик текста (в нём мы можем сделать любую удобную нам предобработку текста: удалить редкие слова, удалить знаки препинания, оставить только слова из определенного списка и т.д.). Токенизатор возвращает текст, разбитый на токены, т.е. на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    return [elem for elem in s.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее интересный для нас с точки зрения хэширования - столбец att9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy['att9'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для применения HashingVectorize выбираем из столбца все различные значения(слова) без повторений, обучаем HashingVectorizer на этих словах и применяем ко всему столбцу. В итоге мы получаем разреженную матрицу. С ней умеют работать многие алгоритмы машинного обучения, но при желании можем перевести ее в numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "coder = HashingVectorizer(tokenizer=my_tokenizer, n_features=3, norm='l2', alternate_sign=True)\n",
    "\n",
    "train_wo_duple = X_copy['att9'].drop_duplicates()\n",
    "coder.fit(train_wo_duple)\n",
    "\n",
    "coder.transform(X_copy['att9'].values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заполнение пропусков**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходных данных могут быть пропущенные значения. Большинство методов машинного обучения не умеют с ними работать. Для этого мы должны каким-нибудь образом заполнить эти пропуски.\n",
    "\n",
    "Способы заполнения пропусков:\n",
    "\n",
    "* средним значением \n",
    "* медианой\n",
    "* самым часто встречающимся значением\n",
    "* каким-то одним новым значением (иногда пропуск в данных можно воспринимать как еще одно категориальное значение).\n",
    "* можно взять часть данных для обучения, а на другой части данных предсказать пропущенные значения (тогда для решения основной задачи нельзя обучаться на первой части данных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, есть ли пропуски в наших данных и много ли их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#индексы строк с NAN\n",
    "\n",
    "X_real = X_copy[X_copy.columns[~cat_features_mask]]\n",
    "\n",
    "print(np.any(np.isnan(X_real)))\n",
    "\n",
    "np.array(pd.isnull(X_real).any(1)).nonzero()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним пропуски в данных с помощью встроенного метода Imputer, используя среднее значение признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mis_replacer = SimpleImputer(strategy=\"mean\")\n",
    "X_no_mis = pd.DataFrame(data=mis_replacer.fit_transform(X_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_no_mis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Масштабирование признаков**\n",
    "\n",
    "Для применения в особенности линейных методов машинного обучения масштабирование признаков очень важно. Может быть так, что метод даст совершенно неправильный результат без масштабирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Масштабирование признаков можно выполнить, например, одним из следующих способов:\n",
    " - $x_{new} = \\dfrac{x - \\mu}{\\sigma}$, где $\\mu, \\sigma$ — среднее и стандартное отклонение значения признака по всей выборке (StandardScaler в sklearn)\n",
    " - $x_{new} = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$, где $[x_{min}, x_{max}]$ — минимальный интервал значений признака (MinMaxScaler в sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.StandardScaler()\n",
    "X_standard_scaled = normalizer.fit_transform(X_no_mis)\n",
    "X_standard_scaled_pd = pd.DataFrame(data=X_standard_scaled)\n",
    "X_standard_scaled_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "X_mm_scaled = mm_scaler.fit_transform(X_no_mis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем пример данных, когда масштабирование признаков сильно влияет на результат работы алгоритмов машинного обучения. Рассмотрим точки, равномерно нанесенные на плоскость (в данном датасете в качестве признаков x и y выступают координаты - долгота и широта точек, все точки территориально находятся в Москве)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('scaler_example.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядят точки из датасета. Точки относятся к двум классам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "Xtrain = X[['lat','lon']].values\n",
    "Ytrain = X['class'].values\n",
    "\n",
    "figure(figsize=(10,10))\n",
    "scatter(Xtrain[Ytrain==1][:,0], Xtrain[Ytrain==1][:,1], color='green')\n",
    "scatter(Xtrain[Ytrain==0][:,0], Xtrain[Ytrain==0][:,1], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "def plot_decision_line(Xtrain, Ytrain, clf, h):\n",
    "\n",
    "    x_min, x_max = Xtrain[:,0].min() - 0.01, Xtrain[:,0].max() + 0.01\n",
    "    y_min, y_max = Xtrain[:,1].min() - 0.01, Xtrain[:,1].max() + 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.contourf(xx, yy, Z)\n",
    "\n",
    "    Ytrain = np.array(Ytrain)\n",
    "    plt.scatter(Xtrain[Ytrain==1][:,0], Xtrain[Ytrain==1][:,1], color='green')\n",
    "    plt.scatter(Xtrain[Ytrain==0][:,0], Xtrain[Ytrain==0][:,1], color='red')\n",
    "\n",
    "    a = clf.coef_[0][0]\n",
    "    b = clf.coef_[0][1]\n",
    "    c = clf.intercept_\n",
    "\n",
    "    K = -a * 1. / b\n",
    "    B = -c * 1. / b\n",
    "\n",
    "    xx0 = np.linspace(x_min, x_max)\n",
    "    yy0 = K * xx0 + B\n",
    "\n",
    "    plt.scatter(xx0, yy0)\n",
    "    plt.show()\n",
    "    \n",
    "plot_decision_line(Xtrain, Ytrain, clf, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Мы видим, что SVM абсолютно не справился с задачей. Попробуем теперь масштабировать данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "TrainScaled = sc.fit_transform(Xtrain)\n",
    "\n",
    "clf.fit(TrainScaled, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_line(TrainScaled, Ytrain, clf, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Работа с текстовыми признаками**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из направлений машинного обучения является работа с текстами и извлечение полезной информации из текстов. Чтобы алгоритмы машинного обучения могли работать с текстами, необходимо перевести тексты в наборы чисел. Для этого применяют различные алгоритмы векторизации текстов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем изучать датасет, содержащий отзывы о фильмах. Отзывы могут быть положительные, либо отрицательные. Наша конечная задача - научиться различать положительные и отрицательные отзывы.\n",
    "\n",
    "Загрузим датасет и уберем из него плохие строки (в которых нет оценки фильму)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open('imdb_labelled.txt', encoding='utf-8') as thefile:\n",
    "    print(thefile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "bad = 0\n",
    "with codecs.open('imdb_labelled.txt', encoding='utf-8') as thefile:\n",
    "    for row in tqdm(thefile.readlines()):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_responses = list(filter(lambda review: 'awful' in review, X))\n",
    "print(bad_responses[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые этапы обработки текста:\n",
    "\n",
    "* приведение к нижнему регистру\n",
    "\n",
    "* удаление пунктуации\n",
    "\n",
    "* удаление всех символов, кроме символов нашего алфавита (в данном случае, латинского)\n",
    "\n",
    "Удалить пунктуацию можно при помощи регулярных выражений. [Прикольный сайт для создания регулярок](https://regex101.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(r'[^\\w\\s]', '', bad_responses[1].lower()).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = [re.sub(r'[^\\w\\s]', '', elem.lower()).strip() for elem in X]\n",
    "Texts[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение ответов в наших данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 способ векторизации: счётчик (CountVectorizer)**\n",
    "\n",
    "Каждому слову соответствует количество его вхождений в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(Texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.transform(Texts[:1]).indptr)\n",
    "print(vectorizer.transform(Texts[:1]).indices)\n",
    "print(vectorizer.transform(Texts[:1]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 способ векторизации: TF-IDF**\n",
    "\n",
    "Ещё один способ работы с текстовыми данными — TF-IDF (Term Frequency–Inverse Document Frequency). Рассмотрим коллекцию текстов $D$. Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:\n",
    "\n",
    "1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте: \n",
    "    $$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$ где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n",
    "2. Inverse Document Frequency $$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$ где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$.\n",
    "\n",
    "Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину: $$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n",
    "\n",
    "Отметим, что значение $\\text{tf}(t, d)$ корректируется для часто встречающихся общеупотребимых слов при помощи значения $\\text{idf}(t, D).$\n",
    "\n",
    "Признаковым описанием одного объекта $d \\in D$ будет вектор $\\bigg(\\text{tf-idf}(t,d, D)\\bigg)_{t\\in V}$, где $V$ – словарь всех слов, встречающихся в коллекции $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(Texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.transform(Texts[:1]).indptr)\n",
    "print(vectorizer.transform(Texts[:1]).indices)\n",
    "print(vectorizer.transform(Texts[:1]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим два рассмотренных метода векторизации к задаче классификации отзывов на два класса (положительные и отрицательные)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(Texts)\n",
    "\n",
    "X = vectorizer.transform(Texts)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict_proba(X_test)[:,1]\n",
    "print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y_test, preds), accuracy_score(y_test, (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(Texts)\n",
    "\n",
    "X = vectorizer.transform(Texts)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict_proba(X_test)[:,1]\n",
    "print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y_test, preds), accuracy_score(y_test, (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важность признаков**\n",
    "\n",
    "В задачах, связанных с обработкой текстов, признаки как правило хорошо интерпретируемы. Для визуального контроля качества работы алгоритма можно посмотреть на те слова, которые алгоритм посчитал наиболее важными для данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = zip(vectorizer.get_feature_names(), lr.coef_[0])\n",
    "weights = sorted(weights, key=lambda i: i[1])\n",
    "for i in range(1,20):\n",
    "    print('%s, %.2f' % weights[-i])\n",
    "    \n",
    "print('...')\n",
    "for i in reversed(range(1,20)):\n",
    "    print('%s, %.2f' % weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 способ векторизации: Word2Vec**\n",
    "\n",
    "Word2Vec - это алгоритм, который собирает статистику по совместному появлению слов в фразах, а затем с помощью нейронных сетей решает задачу снижения размерности и выдает на выходе компактные векторные представления слов, в максимальной степени отражающие отношения этих слов в обрабатываемых текстах.\n",
    "\n",
    "Нахождение связей между контекстами слов основано на предположении, что слова, находящиеся в похожих контекстах, имеют тенденцию значить похожие вещи, т.е. быть семантически близкими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим векторизацию с помощью word2vec для наших данных. Кроме того, удалим stop-слова, то есть слова, часто встречающиеся во всех английских текстах - это ещё один полезный метод обработки текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(review, remove_stopwords=True):\n",
    "    \n",
    "    words = review.split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем строки нашего датасета в токенизированный вид и удалим из них стоп-слова - в этом виде они пригодны для использования word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=True ):\n",
    "\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    sentences = []\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(delete_stopwords(sentence,remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "Y = []\n",
    "for i in range(len(Texts)):\n",
    "    if len(set(Texts[i])) == 1:\n",
    "        continue\n",
    "    Y.append(y[i])\n",
    "    sentences += review_to_sentences(Texts[i], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим word2vec к токенизированному корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 512         \n",
    "min_word_count = 3           \n",
    "num_workers = 4       \n",
    "context = 5                                                                          \n",
    "downsampling = 1e-5   \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=42)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = str(num_features) + \"features_word2vec\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь каждое слово корпуса имеет векторное представление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве одного из способов векторизовать текст (в нашем случае отзыв на фильм), можно усреднить векторы слов, входящих в этот текст. Так и сделаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word_set = set(model.wv.index2word)\n",
    "\n",
    "def normalize(x):\n",
    "    return x / np.sqrt(np.dot(x, x))\n",
    "\n",
    "def make_featurevec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec += model[word]\n",
    "    if nwords > 0:\n",
    "        featureVec = normalize(featureVec)\n",
    "        # featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def get_avg_featurevecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = make_featurevec(review, model, num_features)\n",
    "        counter += 1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "trainDataVecs = get_avg_featurevecs(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, обучим классификатор на полученных признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trainDataVecs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict_proba(X_test)[:,1]\n",
    "print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y_test, preds), accuracy_score(y_test, (preds > 0.5).astype(int))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
